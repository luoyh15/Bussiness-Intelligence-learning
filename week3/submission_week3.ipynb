{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking1: 如何使用用户标签来指导业务（如何提升业务）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 精准营销。通过用户标签匹配用户更有可能感兴趣的商品或服务，获取新客户。\n",
    "2. 个性化推荐。根据用户标签进行个性化推荐，让用户能够获取到更感兴趣的内容，提升用户使用时长。\n",
    "3. 流失率预测。在关键节点，定向推送优惠信息，留存用户。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking2:如果给你一堆用户数据，没有打标签。你该如何处理（如何打标签）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 请人工打标签。比较贵，但质量好。\n",
    "2. 自动打标签。通过聚类，成本低，质量较差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking3: 准确率和精确率有何不同（评估指标）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 准确率：$accuracy=\\frac{TP+TN}{TP+FP+TN+FN}$， 所有预测正确的样本中占总样本的比例\n",
    "- 精确率：$precision=\\frac{TP}{TP+FP}$， 在所有正的样本中，预测正确的比例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking4: 如果你使用大众点评，想要给某个餐厅打标签。这时系统可以自动提示一些标签，你会如何设计（标签推荐）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 菜品相关：种类、口味、价位区间\n",
    "- 服务水平：服务及时\n",
    "- 就餐环境：是否可订座、有无包厢、有无车位、有无大桌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking5: 我们今天使用了10种方式来解MNIST，这些方法有何不同？你还有其他方法来解决MNIST识别问题么（分类方法）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression：根据每个像素点的权重来分类。\n",
    "- Cart，ID3：树模型，会根据某一些重要的像素点进行分类。\n",
    "- LDA：(Linear Discriminant Analysis)一种监督学习的降维技术。\n",
    "- Naive Bayes:直接找出特征输出Y和特征X的联合分布。需要特征之间彼此独立。\n",
    "- SVM: 最大化分类间隔，相比于LR，效果更好。\n",
    "- KNN：找训练集中最匹配的K个样本作为分类依据。\n",
    "- AdaBoost：集成学习，用boosting思想，自适应设置各弱分类器的权重。\n",
    "- XGBoost：GBDT的高效实现，性能提升、精度提升，支持的弱学习器不仅局限于决策树\n",
    "- TPOT：自动机器学习，需要遍历各种模型及参数，准确率高，但效率较低。\n",
    "- keras：深度模型。图象识别一般用CNN，能够高效提取边缘等图象特征，如果加入anchor可解决大小、旋转敏感问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action1：针对Delicious数据集，对SimpleTagBased算法进行改进（使用NormTagBased、TagBased-TFIDF算法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import operator\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始数据加载...\n",
      "数据集大小为 437593.\n",
      "设置tag的人数 1867.\n",
      "数据加载完成\n",
      "\n",
      "训练集样本数 1860, 测试集样本数 1793\n"
     ]
    }
   ],
   "source": [
    "# 使用SimpleTagBased算法对Delicious2K数据进行推荐\n",
    "# 原始数据集：https://grouplens.org/datasets/hetrec-2011/\n",
    "# 数据格式：userID     bookmarkID     tagID     timestamp\n",
    "file_path = \"./user_taggedbookmarks-timestamps.dat\"\n",
    "# 字典类型，保存了user对item的tag，即{userid: {item1:[tag1, tag2], ...}}\n",
    "records = defaultdict(lambda: defaultdict(list))        \n",
    "# 数据加载\n",
    "def load_data():\n",
    "    print(\"开始数据加载...\")\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    for i in range(len(df)):\n",
    "        uid = df['userID'][i]\n",
    "        iid = df['bookmarkID'][i]\n",
    "        tag = df['tagID'][i]      \n",
    "        records[uid][iid].append(tag)\n",
    "    print(\"数据集大小为 %d.\" % (len(df)))\n",
    "    print(\"设置tag的人数 %d.\" % (len(records)))\n",
    "    print(\"数据加载完成\\n\")\n",
    "\n",
    "# 将数据集拆分为训练集和测试集\n",
    "# 训练集，测试集\n",
    "train_data = defaultdict(lambda: defaultdict(list))\n",
    "test_data = defaultdict(lambda: defaultdict(list))\n",
    "def train_test_split(ratio, seed=100):\n",
    "    random.seed(seed)\n",
    "    for u in records.keys():\n",
    "        for i in records[u].keys():\n",
    "            # ratio比例设置为测试集\n",
    "            if random.random()<ratio:\n",
    "                for t in records[u][i]:\n",
    "                    test_data[u][i].append(t)\n",
    "            else:\n",
    "                for t in records[u][i]:\n",
    "                    train_data[u][i].append(t)        \n",
    "    print(\"训练集样本数 %d, 测试集样本数 %d\" % (len(train_data),len(test_data)))\n",
    "\n",
    "# 数据加载\n",
    "load_data()\n",
    "# 训练集，测试集拆分，20%测试集\n",
    "train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_tags, tag_items, user_items初始化完成.\n",
      "user_tags大小 1860, tag_items大小 36884, user_items大小 1860\n",
      "tag_users大小 36884, item_tags大小 59555, item_users大小 59555\n"
     ]
    }
   ],
   "source": [
    "# 使用训练集，初始化user_tags, tag_items, user_items\n",
    "# 用户标签，商品标签\n",
    "user_tags = defaultdict(lambda: defaultdict(int))\n",
    "tag_items = defaultdict(lambda: defaultdict(int))\n",
    "user_items = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "item_users = defaultdict(lambda: defaultdict(int))\n",
    "item_tags = defaultdict(lambda: defaultdict(int))\n",
    "tag_users = defaultdict(lambda: defaultdict(int))\n",
    "def initStat():\n",
    "    records=train_data\n",
    "    for u,items in records.items():\n",
    "        for i,tags in items.items():\n",
    "            for tag in tags:\n",
    "                #print tag\n",
    "                # 用户和tag的关系\n",
    "                user_tags[u][tag] += 1\n",
    "                # tag和item的关系\n",
    "                tag_items[tag][i] += 1\n",
    "                # 用户和item的关系\n",
    "                user_items[u][i] += 1\n",
    "                # tag和user的关系\n",
    "                tag_users[tag][u] += 1\n",
    "                # item和tag的关系\n",
    "                item_tags[i][tag] += 1\n",
    "                # item和user的关系\n",
    "                item_users[i][u] += 1\n",
    "    print(\"user_tags, tag_items, user_items初始化完成.\")\n",
    "    print(\"user_tags大小 %d, tag_items大小 %d, user_items大小 %d\" % (len(user_tags), len(tag_items), len(user_items)))\n",
    "    print(\"tag_users大小 %d, item_tags大小 %d, item_users大小 %d\" % (len(tag_users), len(item_tags), len(item_users)))\n",
    "    \n",
    "initStat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对用户user推荐Top-N\n",
    "def recommend(user, N):\n",
    "    recommend_items=defaultdict(int)\n",
    "    # 对Item进行打分，分数为所有的（用户对某标签使用的次数 wut, 乘以 商品被打上相同标签的次数 wti）之和\n",
    "    tagged_items = user_items[user]   \n",
    "    for tag, wut in user_tags[user].items():\n",
    "        #print(self.user_tags[user].items())\n",
    "        for item, wti in tag_items[tag].items():\n",
    "            if item in tagged_items:\n",
    "                continue\n",
    "            #print('wut = %s, wti = %s' %(wut, wti))\n",
    "            recommend_items[item] += wut * wti / norm(user, tag)\n",
    "    return heapq.nlargest(N, recommend_items.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用测试集，计算准确率和召回率\n",
    "def precisionAndRecall(N):\n",
    "    hit = 0\n",
    "    h_recall = 0\n",
    "    h_precision = 0\n",
    "    for user,items in test_data.items():\n",
    "        if user not in train_data:\n",
    "            continue\n",
    "        # 获取Top-N推荐列表\n",
    "        rank = recommend(user, N)\n",
    "        for item,rui in rank:\n",
    "            if item in items:\n",
    "                hit = hit + 1\n",
    "        h_recall += len(items)\n",
    "        h_precision += N\n",
    "    #print('一共命中 %d 个, 一共推荐 %d 个, 用户设置tag总数 %d 个' %(hit, h_precision, h_recall))\n",
    "    # 返回准确率 和 召回率\n",
    "    return (hit/(h_precision*1.0)), (hit/(h_recall*1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用测试集，对推荐结果进行评估\n",
    "def testRecommend():\n",
    "    print(\"推荐结果评估\")\n",
    "    print(\"%3s %10s %10s\" % ('N',\"精确率\",'召回率'))\n",
    "    for n in [5,10,20,40,60,80,100]:\n",
    "        precision,recall = precisionAndRecall(n)\n",
    "        print(\"%3d %10.3f%% %10.3f%%\" % (n, precision * 100, recall * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推荐结果评估\n",
      "  N        精确率        召回率\n",
      "  5      0.829%      0.355%\n",
      " 10      0.633%      0.542%\n",
      " 20      0.512%      0.877%\n",
      " 40      0.381%      1.304%\n",
      " 60      0.318%      1.635%\n",
      " 80      0.276%      1.893%\n",
      "100      0.248%      2.124%\n"
     ]
    }
   ],
   "source": [
    "# SimpleTagBased\n",
    "def norm(user, tag):\n",
    "    return 1  \n",
    "testRecommend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推荐结果评估\n",
      "  N        精确率        召回率\n",
      "  5      0.907%      0.388%\n",
      " 10      0.638%      0.546%\n",
      " 20      0.507%      0.868%\n",
      " 40      0.356%      1.218%\n",
      " 60      0.287%      1.476%\n",
      " 80      0.255%      1.750%\n",
      "100      0.241%      2.061%\n"
     ]
    }
   ],
   "source": [
    "# NormTagBased-1\n",
    "def norm(user, tag):\n",
    "    return len(user_tags[user].items())*len(tag_users[tag].items())\n",
    "testRecommend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推荐结果评估\n",
      "  N        精确率        召回率\n",
      "  5      0.806%      0.345%\n",
      " 10      0.577%      0.494%\n",
      " 20      0.428%      0.733%\n",
      " 40      0.300%      1.026%\n",
      " 60      0.259%      1.333%\n",
      " 80      0.237%      1.620%\n",
      "100      0.222%      1.903%\n"
     ]
    }
   ],
   "source": [
    "# NormTagBased-2\n",
    "def norm(user, tag):\n",
    "    return len(user_tags[user].items())*len(tag_items[tag].items())\n",
    "testRecommend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推荐结果评估\n",
      "  N        精确率        召回率\n",
      "  5      1.008%      0.431%\n",
      " 10      0.761%      0.652%\n",
      " 20      0.549%      0.940%\n",
      " 40      0.402%      1.376%\n",
      " 60      0.328%      1.687%\n",
      " 80      0.297%      2.033%\n",
      "100      0.269%      2.306%\n"
     ]
    }
   ],
   "source": [
    "# TagBased-TFIDF\n",
    "from math import log\n",
    "def norm(user, tag):\n",
    "    return log(1+len(tag_users[tag].items()))\n",
    "testRecommend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action2: 对Titanic数据进行清洗，建模并对乘客生存进行预测。使用之前介绍过的10种模型中的至少2种（包括TPOT）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "train_data = pd.read_csv('./Titanic_Data-master/train.csv')\n",
    "test_data = pd.read_csv('./Titanic_Data-master/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Age的缺失值用平均值填充\n",
    "train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)\n",
    "test_data['Age'].fillna(test_data['Age'].mean(), inplace=True)\n",
    "# Embarked:训练集中有两个缺失值用众数填充，测试集没有缺失\n",
    "embarked_count = train_data['Embarked'].value_counts()\n",
    "train_data['Embarked'].fillna('S', inplace=True)\n",
    "# Fare：测试集中有一个缺失值，用平均值填充\n",
    "test_data['Fare'].fillna(test_data['Fare'].mean(), inplace=True)\n",
    "# Cabin缺失值太多直接舍弃\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "train_features = train_data[features]\n",
    "test_features = test_data[features]\n",
    "train_labels = train_data['Survived']\n",
    "# 进行one-hot编码\n",
    "dvec = DictVectorizer(sparse=False)\n",
    "train_features = dvec.fit_transform(train_features.to_dict(orient='records'))\n",
    "test_features = dvec.transform(test_features.to_dict(orient='records'))\n",
    "# 训练集验证集切割\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    train_features, train_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# 采用Z-Score规范化，有些模型需要归一化输入\n",
    "ss = StandardScaler()\n",
    "train_ss_features = ss.fit_transform(train_features)\n",
    "train_ss_x = ss.transform(train_x)\n",
    "test_ss_x = ss.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TPOT自动机器学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735ef602c20f4e878be7fffaf74f6f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Optimization Progress'), FloatProgress(value=0.0, max=1020.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.8316615403929447\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.8316615403929447\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.8339087314041805\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.8339087314041805\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.8350260498399347\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.8350260498399347\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.8350260498399347\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 11 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 12 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 13 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 14 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 15 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 16 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 17 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 18 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 19 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 20 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 21 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 22 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 23 - Current best internal CV score: 0.8428912183792606\n",
      "\n",
      "Generation 24 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 25 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 26 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 27 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 28 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 29 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 30 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 31 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 32 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 33 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 34 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 35 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 36 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 37 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 38 - Current best internal CV score: 0.8428974954491244\n",
      "\n",
      "Generation 39 - Current best internal CV score: 0.8440148138848785\n",
      "\n",
      "Generation 40 - Current best internal CV score: 0.8440148138848785\n",
      "\n",
      "Generation 41 - Current best internal CV score: 0.8440148138848785\n",
      "\n",
      "Generation 42 - Current best internal CV score: 0.8440148138848785\n",
      "\n",
      "Generation 43 - Current best internal CV score: 0.8440148138848785\n",
      "\n",
      "Generation 44 - Current best internal CV score: 0.8440148138848785\n",
      "\n",
      "Generation 45 - Current best internal CV score: 0.8440148138848785\n",
      "\n",
      "Generation 46 - Current best internal CV score: 0.8440148138848785\n",
      "\n",
      "Generation 47 - Current best internal CV score: 0.8440148138848785\n",
      "\n",
      "Generation 48 - Current best internal CV score: 0.8440148138848785\n",
      "\n",
      "Generation 49 - Current best internal CV score: 0.8440148138848785\n",
      "\n",
      "Generation 50 - Current best internal CV score: 0.8440148138848785\n",
      "\n",
      "Best pipeline: RandomForestClassifier(StandardScaler(LinearSVC(input_matrix, C=0.001, dual=True, loss=squared_hinge, penalty=l2, tol=0.0001)), bootstrap=False, criterion=entropy, max_features=0.55, min_samples_leaf=5, min_samples_split=4, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(generations=50, population_size=20, random_state=42, verbosity=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TPOT自动机器学习\n",
    "from tpot import TPOTClassifier\n",
    "# tpot采用遗传算法，generations：遗传几代；population_size：每代数目。\n",
    "pipeline_optimizer = TPOTClassifier(generations=50, population_size=20, cv=5,\n",
    "                                    random_state=42, verbosity=2)\n",
    "pipeline_optimizer.fit(train_features, train_labels)\n",
    "# 最终得到的最好的模型是随机森林，基分类器是线性SVM，参数如下：\n",
    "# RandomForestClassifier(\n",
    "#     StandardScaler(LinearSVC(input_matrix, C=0.001, dual=True, loss=squared_hinge, penalty=l2, tol=0.0001)), \n",
    "#     bootstrap=False, criterion=entropy, max_features=0.55, \n",
    "#     min_samples_leaf=5, min_samples_split=4, n_estimators=100)\n",
    "# cv准确率0.844\n",
    "pipeline_optimizer.export('tpot_model.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 决策树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树模型交叉验证准确率： 0.7766932395957568\n"
     ]
    }
   ],
   "source": [
    "# 决策树模型\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(train_x, train_y)\n",
    "pred_labels_cart = dt.predict(test_features)\n",
    "print('决策树模型交叉验证准确率：', np.mean(cross_val_score(dt, train_features, train_labels, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost交叉验证准确率: 0.8305630531667818\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "bst = XGBClassifier(\n",
    "        learning_rate=0.01,#学习率\n",
    "        n_estimators=3000,#步长\n",
    "        max_depth=4,#深度\n",
    "        objective='binary:logistic',\n",
    "        seed=27,\n",
    "        eval_metric = 'auc',\n",
    "        tree_mothod='hist'\n",
    "    )\n",
    "bst.fit(train_x,train_y)\n",
    "#预测结果\n",
    "pred_labels_xgb = bst.predict(test_x)\n",
    "# 交叉验证准确率\n",
    "print('XGBoost交叉验证准确率:', np.mean(cross_val_score(bst, train_features, train_labels, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 逻辑回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR交叉验证准确率: 0.7856506182913816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_ss_x, train_y)\n",
    "predict_labels_lr = lr.predict(test_ss_x)\n",
    "# 交叉验证准确率\n",
    "print('LR交叉验证准确率:', np.mean(cross_val_score(lr, train_ss_features, train_labels, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LDA分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA准确率: 0.7946142740568702\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  \n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(train_ss_x,train_y)\n",
    "predict_labels_lda=lda.predict(test_ss_x)\n",
    "# 交叉验证准确率\n",
    "print('LDA交叉验证准确率:', np.mean(cross_val_score(lda, train_ss_features, train_labels, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朴素贝叶斯交叉验证准确率: 0.7890276818780995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(train_x,train_y)\n",
    "predict_labels_nb = nb.predict(test_x)\n",
    "# 交叉验证准确率\n",
    "print('朴素贝叶斯交叉验证准确率:',  np.mean(cross_val_score(nb, train_features, train_labels, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM交叉验证准确率: 0.8249074132195091\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='rbf', C=1.0, gamma='auto')\n",
    "svc.fit(train_ss_x,train_y)\n",
    "predict_labels_svc=svc.predict(test_ss_x)\n",
    "# 交叉验证准确率\n",
    "print('SVM交叉验证准确率:',  np.mean(cross_val_score(svc, train_ss_features, train_labels, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN交叉验证准确率: 0.8036093151716777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_ss_x,train_y)\n",
    "predict_labels_knn=knn.predict(test_ss_x)\n",
    "# 交叉验证准确率\n",
    "print('KNN交叉验证准确率:', np.mean(cross_val_score(knn, train_ss_features, train_labels, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaBoost分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost交叉验证准确率: 0.8013684012303057\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import  AdaBoostClassifier \n",
    "# 弱分类器\n",
    "dt_stump = DecisionTreeClassifier(max_depth=5,min_samples_leaf=1)\n",
    "dt_stump.fit(train_ss_x, train_y)\n",
    "#dt_stump_err = 1.0-dt_stump.score(test_x, test_y)\n",
    "# 设置AdaBoost迭代次数\n",
    "n_estimators=500\n",
    "abst = AdaBoostClassifier(base_estimator=dt_stump,n_estimators=n_estimators)\n",
    "abst.fit(train_ss_x,train_y)\n",
    "predict_labels_abst=abst.predict(test_ss_x)\n",
    "# 交叉验证准确率\n",
    "print('AdaBoost交叉验证准确率:',  np.mean(cross_val_score(abst, train_ss_features, train_labels, cv=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
