{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinking1: 在实际工作中，FM和MF哪个应用的更多，为什么**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MF只考虑user、item两个维度，FM可以处理更多特征，MF是FM的特例。\n",
    "- 一般FM应用更多，由于FM考虑了更多特征，而且考虑了特征之间的相关性，所以预测结果更为准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinking2：FFM与FM有哪些区别？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FFM是带场的FM，对于每个特征有多个隐向量。\n",
    "- FM是FFM的特例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinking3：DeepFM相比于FM解决了哪些问题，原理是怎样的**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DeepFM用深度模型DNN来处理三阶及以上的特征交叉，用FM处理一阶和二阶特征交叉。\n",
    "- 既利用了FM的优点，又规避了FM处理高阶特征计算量大的不足。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinking4：Surprise工具中的baseline算法原理是怎样的？BaselineOnly和KNNBaseline有什么区别？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Baseline算法：基于统计的基准预测线打分，预测值=用户的基准+商品的基准\n",
    " - 预测值$\\hat{r}_{ui} = b_{ui}$\n",
    " - $b_{ui} = b_u+b_i$: 用户对整体的偏差+商品对整体的偏差\n",
    " - 用ALS进行计算\n",
    "- KNNBaseline算法：KNN+Baseline\n",
    " - 预测值$\\hat{r}_{ui} = b_{ui}+用户领域/商品领域$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinking5：基于邻域的协同过滤都有哪些算法，请简述原理**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于用户领域的协同过滤（UserCF）：推荐与目标用户相似的用户群体感兴趣的物品\n",
    "- 基于物品领域的协同过滤（ItemCF）：推荐与目标用户喜欢的物品相似的商品  \n",
    "\n",
    "以上两种方法有不同的适用场景，商品迭代快用UserCF；商品比较固定，用ItemCF。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action1：使用libfm工具对movielens进行评分预测，采用SGD优化算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据转换\n",
    ">./triple_format_to_libfm.pl -in ratings.dat -target 2 -delete_column 3 -separator \"::\"\n",
    "\n",
    "用SGD进行迭代计算：learning rate=0.01\n",
    "> ./libFM -task r -train ratings.dat.libfm -test ratings.dat.libfm -dim '1,1,8' -iter 100 -method sgd -learn_rate 0.01 -regular '0,0,0.01' -init_stdev 0.1 -out movielens_out.txt\n",
    "\n",
    "迭代结果:\n",
    "> #Iter= 99       Train=0.778209  Test=0.778209   \n",
    "> Final   Train=0.778209  Test=0.778209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action2：使用DeepFM对movielens进行评分预测**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对movielens_sample进行评分预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from deepctr.models import DeepFM\n",
    "from deepctr.feature_column import SparseFeat,get_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据加载\n",
    "path = 'DeepCTR/'\n",
    "data = pd.read_csv(path+\"movielens_sample.txt\")\n",
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对特征标签进行编码\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature])\n",
    "# 计算每个特征中的 不同特征值的个数\n",
    "fixlen_feature_columns = [SparseFeat(feature, data[feature].nunique()) for feature in sparse_features]\n",
    "# print(fixlen_feature_columns)\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集切分成训练集和测试集\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train_model_input = {name:train[name].values for name in feature_names}\n",
    "test_model_input = {name:test[name].values for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 12.7820 - mse: 12.7820 - val_loss: 17.2466 - val_mse: 17.2466\n",
      "Epoch 2/6\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.4843 - mse: 11.4843 - val_loss: 15.3015 - val_mse: 15.3015\n",
      "Epoch 3/6\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.4630 - mse: 9.4630 - val_loss: 12.2500 - val_mse: 12.2500\n",
      "Epoch 4/6\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.4596 - mse: 6.4596 - val_loss: 7.9932 - val_mse: 7.9932\n",
      "Epoch 5/6\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.0271 - mse: 3.0271 - val_loss: 3.4770 - val_mse: 3.4770\n",
      "Epoch 6/6\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.3160 - mse: 1.3160 - val_loss: 1.2835 - val_mse: 1.2835\n"
     ]
    }
   ],
   "source": [
    "# 使用DeepFM进行训练\n",
    "model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')\n",
    "model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "history = model.fit(train_model_input, train[target].values, batch_size=16, epochs=6, verbose=True, validation_split=0.2, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这里由于训练集很小只有128，所以batch size应该不能取256，不然就是全局梯度下降了。这里取batch size=16，效果更好**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE 1.0165136496870075\n"
     ]
    }
   ],
   "source": [
    "# 使用DeepFM进行预测\n",
    "pred_ans = model.predict(test_model_input, batch_size=16)\n",
    "# 输出RMSE或MSE\n",
    "mse = round(mean_squared_error(test[target].values, pred_ans), 4)\n",
    "rmse = mse ** 0.5\n",
    "print(\"test RMSE\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对movielens 100K数据集进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from deepctr.models import DeepFM\n",
    "from deepctr.feature_column import SparseFeat,get_feature_names, DenseFeat\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据加载\n",
    "path = 'DeepCTR/ml-100k/'\n",
    "# load origin data of movielens 100K\n",
    "u_data = pd.read_csv('DeepCTR/ml-100k/u.data', header=None, sep='\\t')\n",
    "u_user = pd.read_csv('DeepCTR/ml-100k/u.user', header=None, sep='|')\n",
    "u_item = pd.read_csv('DeepCTR/ml-100k/u.item', header=None, sep='|', encoding='unicode_escape')\n",
    "# get the columns name \n",
    "u_data.columns = 'user_id | item_id | rating | timestamp'.split(' | ')\n",
    "u_user.columns = 'user_id | age | gender | occupation | zip_code'.split(' | ')\n",
    "item_columns = 'movie_id | movie_title | release_date | video_release_date | IMDb_URL | unknown | Action | Adventure | Animation | Children_s | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western'\n",
    "u_item.columns = item_columns.split(' | ')\n",
    "# merge the three tables \n",
    "ml_data = pd.merge(u_data, u_user, on=\"user_id\")\n",
    "ml_data = pd.merge(ml_data, u_item, left_on='item_id', right_on='movie_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "# timestamp convert to date form same as release date\n",
    "def timestamp2date(ts):\n",
    "    return datetime.utcfromtimestamp(ts).strftime('%d-%b-%Y')\n",
    "\n",
    "def timestamp2hour(ts):\n",
    "    return datetime.utcfromtimestamp(ts).hour\n",
    "\n",
    "def str2date(s):\n",
    "    return datetime.strptime(s, '%d-%b-%Y')\n",
    "\n",
    "ml_data['rate_hour'] = ml_data['timestamp'].map(timestamp2hour)\n",
    "ml_data['rate_hour'] = pd.cut(ml_data['rate_hour'], 3, labels=['moring', 'afternoon', 'night'])\n",
    "ml_data['rate_date'] = ml_data['timestamp'].map(timestamp2date)\n",
    "\n",
    "# 处理空值\n",
    "ml_data['release_date'] = ml_data['release_date'].fillna('')\n",
    "# 上映与观看的间隔时间\n",
    "def delta_days(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return -1\n",
    "    return (str2date(s1)-str2date(s2)).days\n",
    "ml_data['delta_days'] = ml_data.apply(lambda x: delta_days(x.rate_date, x.release_date), axis=1)\n",
    "\n",
    "#处理年龄数据\n",
    "# ml_data['age_label'] = pd.cut(ml_data['age'], 3, labels=['young', 'middle', 'old'])\n",
    "ml_data['age_label'] = pd.cut(ml_data['age'], 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对稀疏类别特征标签进行类别编码\n",
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"occupation\", \"zip_code\", \"age_label\", \"rate_hour\"]\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    ml_data[feature] = lbe.fit_transform(ml_data[feature])\n",
    "    \n",
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=ml_data[feat].nunique(),embedding_dim=8)\n",
    "                       for feat in sparse_features]\n",
    "# 单独对timestamp和release date进行处理\n",
    "lbe = LabelEncoder()\n",
    "lbe.fit(pd.concat([ml_data['rate_date'],ml_data['release_date']]))\n",
    "ml_data['rate_date'] = lbe.transform(ml_data['rate_date'])\n",
    "ml_data['release_date'] = lbe.transform(ml_data['release_date'])\n",
    "vocabulary_size = pd.concat([ml_data['rate_date'],ml_data['release_date']]).nunique()\n",
    "\n",
    "sparse_features += ['rate_date', 'release_date'] \n",
    "fixlen_feature_columns += [SparseFeat(feat, vocabulary_size=vocabulary_size,embedding_dim=8)\n",
    "                       for feat in ['rate_date', 'release_date']]\n",
    "# 对稠密特征进行归一化\n",
    "# 观看与上映时间间隔作为稠密特征\n",
    "dense_features = [\"delta_days\"]\n",
    "mms = MinMaxScaler(feature_range=(0,1))\n",
    "ml_data[dense_features] = mms.fit_transform(ml_data[dense_features])\n",
    "#\n",
    "dense_features += list(u_item.columns[5:])\n",
    "fixlen_feature_columns += [DenseFeat(feat, 1,) for feat in dense_features]\n",
    "# 目标标签\n",
    "target = ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成特征列\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集切分成训练集和测试集\n",
    "train, test = train_test_split(ml_data, test_size=0.2)\n",
    "\n",
    "train_model_input = {name:train[name].values for name in feature_names}\n",
    "test_model_input = {name:test[name].values for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luoyh\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 2s 7ms/step - loss: 467.9244 - mse: 3.2478 - val_loss: 53.4850 - val_mse: 1.4016\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 15.0920 - mse: 1.4092 - val_loss: 2.2361 - val_mse: 1.1965\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.4765 - mse: 1.2391 - val_loss: 1.0973 - val_mse: 1.0859\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.1343 - mse: 1.1314 - val_loss: 1.0286 - val_mse: 1.0278\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 1.0579 - mse: 1.0572 - val_loss: 0.9933 - val_mse: 0.9925\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.0069 - mse: 1.0060 - val_loss: 0.9689 - val_mse: 0.9678\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.9676 - mse: 0.9665 - val_loss: 0.9521 - val_mse: 0.9509\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.9373 - mse: 0.9360 - val_loss: 0.9385 - val_mse: 0.9372\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.9161 - mse: 0.9146 - val_loss: 0.9299 - val_mse: 0.9284\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.8971 - mse: 0.8954 - val_loss: 0.9235 - val_mse: 0.9218\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.8839 - mse: 0.8820 - val_loss: 0.9184 - val_mse: 0.9164\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.8743 - mse: 0.8723 - val_loss: 0.9147 - val_mse: 0.9126\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.8646 - mse: 0.8625 - val_loss: 0.9120 - val_mse: 0.9098\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.8583 - mse: 0.8560 - val_loss: 0.9095 - val_mse: 0.9072\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.8532 - mse: 0.8507 - val_loss: 0.9072 - val_mse: 0.9046\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.8493 - mse: 0.8467 - val_loss: 0.9055 - val_mse: 0.9029\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.8467 - mse: 0.8440 - val_loss: 0.9047 - val_mse: 0.9019\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.8439 - mse: 0.8412 - val_loss: 0.9036 - val_mse: 0.9007\n",
      "Epoch 19/20\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.8418 - mse: 0.8390 - val_loss: 0.9032 - val_mse: 0.9003\n",
      "Epoch 20/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.8399 - mse: 0.8369 - val_loss: 0.9028 - val_mse: 0.8998\n"
     ]
    }
   ],
   "source": [
    "# 使用DeepFM进行训练\n",
    "model = DeepFM(linear_feature_columns,dnn_feature_columns,task='regression', \n",
    "               dnn_hidden_units=(64, 32, 64), dnn_dropout=0.6,\n",
    "               l2_reg_embedding=0.3, l2_reg_dnn=10)\n",
    "model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "\n",
    "history = model.fit(train_model_input, train[target].values,\n",
    "                    batch_size=256, epochs=20, verbose=1, validation_split=0.2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE 0.936162379077476\n"
     ]
    }
   ],
   "source": [
    "# 使用DeepFM进行预测\n",
    "pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "# 输出RMSE或MSE\n",
    "mse = round(mean_squared_error(test[target].values, pred_ans), 4)\n",
    "rmse = mse ** 0.5\n",
    "print(\"test RMSE\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这里调整了神经元数目、增大了正则项、加了dropout，防止过拟合**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action3:使用基于邻域的协同过滤（KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline中的任意一种）对MovieLens数据集进行协同过滤，采用k折交叉验证(k=3)，输出每次计算的RMSE, MAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import KFold\n",
    "from surprise import KNNBasic, KNNWithMeans, KNNBaseline, KNNWithZScore\n",
    "# 数据读取\n",
    "path = 'L6-code/knn_cf/'\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
    "data = Dataset.load_from_file(path+'ratings.csv', reader=reader)\n",
    "train_set = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNNBasic with UserCF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9030\n",
      "MAE:  0.6904\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9030\n",
      "MAE:  0.6912\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9057\n",
      "MAE:  0.6921\n"
     ]
    }
   ],
   "source": [
    "# 定义K折交叉验证迭代器, K=3\n",
    "kf = KFold(n_splits=3)\n",
    "# 存储K个模型\n",
    "algos = []\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo = KNNBasic() #use default setting\n",
    "    algos.append(algo)\n",
    "    # 训练并预测\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    # 计算RMSE\n",
    "    accuracy.rmse(predictions, verbose=True)\n",
    "    # 计算MAE\n",
    "    accuracy.mae(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNNBasic with ItemCF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9620\n",
      "MAE:  0.7427\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9591\n",
      "MAE:  0.7404\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9621\n",
      "MAE:  0.7425\n"
     ]
    }
   ],
   "source": [
    "#KNNBasic with item CF\n",
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': False  # compute  similarities between items\n",
    "               }\n",
    "# 存储K个模型\n",
    "algos = []\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo = KNNBasic(sim_options=sim_options)\n",
    "    algos.append(algo)\n",
    "    # 训练并预测\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    # 计算RMSE\n",
    "    accuracy.rmse(predictions, verbose=True)\n",
    "    # 计算MAE\n",
    "    accuracy.mae(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNNWithMeans**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8818\n",
      "MAE:  0.6794\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8809\n",
      "MAE:  0.6784\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8785\n",
      "MAE:  0.6773\n"
     ]
    }
   ],
   "source": [
    "#KNNWithMeans\n",
    "# 存储K个模型\n",
    "algos = []\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo = KNNWithMeans()\n",
    "    algos.append(algo)\n",
    "    # 训练并预测\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    # 计算RMSE\n",
    "    accuracy.rmse(predictions, verbose=True)\n",
    "    # 计算MAE\n",
    "    accuracy.mae(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNNWithZScore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8771\n",
      "MAE:  0.6722\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8809\n",
      "MAE:  0.6749\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8784\n",
      "MAE:  0.6730\n"
     ]
    }
   ],
   "source": [
    "# 存储K个模型\n",
    "algos = []\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo = KNNWithZScore()\n",
    "    algos.append(algo)\n",
    "    # 训练并预测\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    # 计算RMSE\n",
    "    accuracy.rmse(predictions, verbose=True)\n",
    "    # 计算MAE\n",
    "    accuracy.mae(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNNBaseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8589\n",
      "MAE:  0.6588\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8565\n",
      "MAE:  0.6579\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8582\n",
      "MAE:  0.6585\n"
     ]
    }
   ],
   "source": [
    "# 存储K个模型\n",
    "algos = []\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo = KNNBaseline()\n",
    "    algos.append(algo)\n",
    "    # 训练并预测\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    # 计算RMSE\n",
    "    accuracy.rmse(predictions, verbose=True)\n",
    "    # 计算MAE\n",
    "    accuracy.mae(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNNBaseline算法对于这个数据集来说效果最好**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
