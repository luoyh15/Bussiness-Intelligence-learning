**Thinking1：机器学习中的监督学习、非监督学习、强化学习有何区别？**
监督学习有label，强化学习有奖励值，非监督学习没有label也没有奖励值。

**Thinking2：什么是策略网络，价值网络，有何区别？**
策略网络：基于策略，给出了选择的可能性。

价值网络：基于每个状态的评分进行选择

**Thinking3：请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的**
select: 选择子节点中概率最高的，直到未扩展的节点

expansion：对选择的节点进行扩展，增加一个或者多个子节点，然后选择其中一个

simulation：用快速走子策略模拟交互过程，直至叶节点。

back propagation：从叶节点往回更新路径上的每个节点的概率。

**Thinking4：假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑**
用户是Environment，推荐算法是Agent，推荐的信息是Action，用户的点击是Reward。

1. 要实现完全的强化学习推荐需要大量的Environment反馈，个性化推荐需要对不同用户设计不同的Agent，数据量可能不够，需要先将用户分类，按类训练Agent
2. 结合传统的推荐算法：传统推荐算法往往会给出各种维度的推荐信息，比如相关性、多样性、新颖性、实时性等维度；而用户往往是多变的，可以通过强化学习，根据用户实时反馈调整每个维度信息出现的概率，实时调整。

**Thinking5：在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路**
车是Agent，道路和乘客是Environment，速度方向是Action，安全性舒适度是Reward。

目前自动驾驶最主要的在于感知部分，也就是道路环境感知，通过环境制定Action通常是用带规则的路径规划的算法。强化学习来实现自动控制还处于研究之中。

1. Reward的设计：安全性肯定是首要的，实际数据中难以获得负奖励值的事件，需要仿真来获取。舒适度也是要考虑的，可根据车辆的加速度、转弯半径等参数给出。
2. Action的选择：主要是速度、方向两个参数



**Action（五子棋）：
棋盘大小 10 * 10
采用强化学习（策略价值网络），用AI训练五子棋AI**

1. MCTS：

   构建蒙特卡洛搜索树，用UCB算法

2. 随机走子策略：

   用于往下扩展，选取分值最高的节点向下探索

3. 策略价值网络：

   指导MCTS搜索，用深度模型给出当前状态的价值和可能选择的概率



