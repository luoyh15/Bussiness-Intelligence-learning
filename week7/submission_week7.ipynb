{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinking1：在CTR点击率预估中，使用GBDT+LR的原理是什么?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用stacking方法。GBDT做特征构造，LR做分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinking2：Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**结构**：Wide部分，线性模型LR；Deep部分，模型用DNN。  \n",
    "LR模型记忆性能好，而DNN模型提取深层特征，泛化能力好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinking3：在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**串行**：代表模型是NFM  \n",
    "**并行**：代表模型是DeepFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinking4：GBDT和随机森林都是基于树的算法，它们有什么区别？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT：用下一颗树去拟合前几棵树的残差，是Boosting思想，预测结果偏差较小，方差较大。  \n",
    "RF：多个弱分类树组合成一个强分类器，是bagging思想，预测结果偏差较大，方差较小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinking5：item流行度在推荐系统中有怎样的应用**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 热门推荐，解决冷启动问题\n",
    "2. 作为个性化推荐时的一个权重，降低流行度高的item的权重，以增加个性化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action1：使用Wide&Deep模型对movielens进行评分预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from deepctr.models import WDL\n",
    "from deepctr.feature_column import SparseFeat,get_feature_names, DenseFeat\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据加载\n",
    "path = 'WideDeep/ml-100k/'\n",
    "# load origin data of movielens 100K\n",
    "u_data = pd.read_csv(path+'u.data', header=None, sep='\\t')\n",
    "u_user = pd.read_csv(path+'u.user', header=None, sep='|')\n",
    "u_item = pd.read_csv(path+'u.item', header=None, sep='|', encoding='unicode_escape')\n",
    "# get the columns name \n",
    "u_data.columns = 'user_id | item_id | rating | timestamp'.split(' | ')\n",
    "u_user.columns = 'user_id | age | gender | occupation | zip_code'.split(' | ')\n",
    "item_columns = 'movie_id | movie_title | release_date | video_release_date | IMDb_URL | unknown | Action | Adventure | Animation | Children_s | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western'\n",
    "u_item.columns = item_columns.split(' | ')\n",
    "# merge the three tables \n",
    "ml_data = pd.merge(u_data, u_user, on=\"user_id\")\n",
    "ml_data = pd.merge(ml_data, u_item, left_on='item_id', right_on='movie_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "# timestamp convert to date form same as release date\n",
    "def timestamp2date(ts):\n",
    "    return datetime.utcfromtimestamp(ts).strftime('%d-%b-%Y')\n",
    "\n",
    "def timestamp2hour(ts):\n",
    "    return datetime.utcfromtimestamp(ts).hour\n",
    "\n",
    "def str2date(s):\n",
    "    return datetime.strptime(s, '%d-%b-%Y')\n",
    "\n",
    "ml_data['rate_hour'] = ml_data['timestamp'].map(timestamp2hour)\n",
    "ml_data['rate_hour'] = pd.cut(ml_data['rate_hour'], 3, labels=['moring', 'afternoon', 'night'])\n",
    "ml_data['rate_date'] = ml_data['timestamp'].map(timestamp2date)\n",
    "\n",
    "# 处理空值\n",
    "ml_data['release_date'] = ml_data['release_date'].fillna('')\n",
    "# 上映与观看的间隔时间\n",
    "def delta_days(s1, s2):\n",
    "    if not s1 or not s2:\n",
    "        return -1\n",
    "    return (str2date(s1)-str2date(s2)).days\n",
    "ml_data['delta_days'] = ml_data.apply(lambda x: delta_days(x.rate_date, x.release_date), axis=1)\n",
    "\n",
    "#处理年龄数据\n",
    "# ml_data['age_label'] = pd.cut(ml_data['age'], 3, labels=['young', 'middle', 'old'])\n",
    "ml_data['age_label'] = pd.cut(ml_data['age'], 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对稀疏类别特征标签进行类别编码\n",
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"occupation\", \"zip_code\", \"age_label\", \"rate_hour\"]\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    ml_data[feature] = lbe.fit_transform(ml_data[feature])\n",
    "    \n",
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=ml_data[feat].nunique(),embedding_dim=8)\n",
    "                       for feat in sparse_features]\n",
    "# 单独对timestamp和release date进行处理\n",
    "lbe = LabelEncoder()\n",
    "lbe.fit(pd.concat([ml_data['rate_date'],ml_data['release_date']]))\n",
    "ml_data['rate_date'] = lbe.transform(ml_data['rate_date'])\n",
    "ml_data['release_date'] = lbe.transform(ml_data['release_date'])\n",
    "vocabulary_size = pd.concat([ml_data['rate_date'],ml_data['release_date']]).nunique()\n",
    "\n",
    "sparse_features += ['rate_date', 'release_date'] \n",
    "fixlen_feature_columns += [SparseFeat(feat, vocabulary_size=vocabulary_size,embedding_dim=8)\n",
    "                       for feat in ['rate_date', 'release_date']]\n",
    "# 对稠密特征进行归一化\n",
    "# 观看与上映时间间隔作为稠密特征\n",
    "dense_features = [\"delta_days\"]\n",
    "mms = MinMaxScaler(feature_range=(0,1))\n",
    "# mms = StandardScaler()\n",
    "ml_data[dense_features] = mms.fit_transform(ml_data[dense_features])\n",
    "#\n",
    "dense_features += list(u_item.columns[5:])\n",
    "fixlen_feature_columns += [DenseFeat(feat, 1,) for feat in dense_features]\n",
    "# 目标标签\n",
    "target = ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成特征列\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集切分成训练集和测试集\n",
    "train, test = train_test_split(ml_data, test_size=0.2)\n",
    "\n",
    "train_model_input = {name:train[name].values for name in feature_names}\n",
    "test_model_input = {name:test[name].values for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luoyh\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9253 - mse: 3.9252 - val_loss: 1.4576 - val_mse: 1.4573\n",
      "Epoch 2/15\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 1.6259 - mse: 1.6256 - val_loss: 1.1328 - val_mse: 1.1324\n",
      "Epoch 3/15\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 1.2505 - mse: 1.2500 - val_loss: 1.0555 - val_mse: 1.0548\n",
      "Epoch 4/15\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 1.0968 - mse: 1.0960 - val_loss: 1.0007 - val_mse: 0.9998\n",
      "Epoch 5/15\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 1.0170 - mse: 1.0161 - val_loss: 0.9734 - val_mse: 0.9723\n",
      "Epoch 6/15\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.9579 - mse: 0.9568 - val_loss: 0.9532 - val_mse: 0.9519\n",
      "Epoch 7/15\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.9267 - mse: 0.9254 - val_loss: 0.9392 - val_mse: 0.9377\n",
      "Epoch 8/15\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.9018 - mse: 0.9002 - val_loss: 0.9303 - val_mse: 0.9286\n",
      "Epoch 9/15\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.8854 - mse: 0.8836 - val_loss: 0.9250 - val_mse: 0.9232\n",
      "Epoch 10/15\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.8716 - mse: 0.8697 - val_loss: 0.9172 - val_mse: 0.9152\n",
      "Epoch 11/15\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.8622 - mse: 0.8601 - val_loss: 0.9136 - val_mse: 0.9114\n",
      "Epoch 12/15\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.8544 - mse: 0.8521 - val_loss: 0.9112 - val_mse: 0.9088\n",
      "Epoch 13/15\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.8466 - mse: 0.8442 - val_loss: 0.9077 - val_mse: 0.9052\n",
      "Epoch 14/15\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.8414 - mse: 0.8387 - val_loss: 0.9038 - val_mse: 0.9011\n",
      "Epoch 15/15\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.8371 - mse: 0.8343 - val_loss: 0.9029 - val_mse: 0.9001\n"
     ]
    }
   ],
   "source": [
    "# 使用Wide&Deep进行训练\n",
    "model = WDL(linear_feature_columns,dnn_feature_columns,task='regression', \n",
    "               dnn_hidden_units=(16, 16, 16), dnn_dropout=0.6,\n",
    "               l2_reg_embedding=1e-5, l2_reg_dnn=0)\n",
    "model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "\n",
    "history = model.fit(train_model_input, train[target].values,\n",
    "                    batch_size=256, epochs=15, verbose=1, validation_split=0.2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE 0.9411694852681954 0.8858\n"
     ]
    }
   ],
   "source": [
    "# 使用Wide&Deep进行预测\n",
    "pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "# 输出RMSE或MSE\n",
    "mse = round(mean_squared_error(test[target].values, pred_ans), 4)\n",
    "rmse = mse ** 0.5\n",
    "print(\"test RMSE\", rmse, mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**从训练过程看有些过拟合，但测试发现增大L2并不能减小过拟合情况，反而在测试集上效果变差，有些迷惑。。。**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
