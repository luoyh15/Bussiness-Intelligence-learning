### AlphaGo Zero实战

- CNN:将棋盘看作图像，给出当前局势好坏
  - $loss=(z-v)^2-\pi^T*log(p)+c\theta^2$
  - z胜负情况，v神经网络评估值，$\pi$神经网络输出落子概率，$p$MCTS得到的节点概率，$c$正则项
  1. 价值网络：
    - 状态评分, 评估当前局势的好坏
  2. 策略网络
    - 落子的概率分布- MCTS：存储各走子的选择搜索
  - 前期通过MCTS的随机搜索结果进行训练，后期根据策略价值网络进行有指导的MCTS搜索
  
- MCTS:随机搜索确定落子概率，不断展开
  - UCB：$UCB=Q(s,a)+U(s,a), U(s,a)=c*p*\frac{\sqrt{log(\sum_b N(s,b))}}{1+N(s,a)}$
  
- self-play:自我训练进行提升
  - 自我对弈进行策略价值网络的训练。越来越强
  
  
### 强化学习的应用

- 自动驾驶
  - 需要模拟环境（数字孪生）进行训练。
- 搜索场景
  - 购买行为看作状态转移
  - reward是购买动作之前的浏览数
  - 短期损失无法避免，长期能够弥补
- 推荐场景
  - 强化学习和自适应学习
- 智能客服
  - reward为用户反馈